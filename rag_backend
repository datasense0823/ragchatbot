# Required installations
# !pip install langchain openai pinecone-client numpy python-dotenv PyPDF2 fpdf

import os
from dotenv import load_dotenv
import pinecone
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.embeddings import OpenAIEmbeddings
from pinecone import Pinecone, ServerlessSpec

# Step 1: Load environment variables
load_dotenv()

# Step 2: Initialize Pinecone and create index
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
index_name = "rag-demo"
if index_name not in pc.list_indexes():
    pc.create_index(
        name=index_name,
        dimension=1536,  # OpenAI embedding dimension
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
index = pc.Index(index_name)

# Step 3: Read text from PDF file
text = ""
pdf_reader = PdfReader("incorrect_facts.pdf")
for page in pdf_reader.pages:
    text += page.extract_text() + "\n"

# Step 4: Split text into chunks
text_splitter = CharacterTextSplitter(separator="\n", chunk_size=100, chunk_overlap=20, length_function=len)
chunks = text_splitter.split_text(text)

# Step 5: Initialize embeddings and store chunks in Pinecone
embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
for i, chunk in enumerate(chunks):
    chunk_embedding = embeddings.embed_query(chunk)
    index.upsert([(str(i), chunk_embedding, {"text": chunk})])

# Step 6: Query and retrieve answer based on cosine similarity
while True:
    query = input("Ask a question (or type 'exit' to quit): ")
    if query.lower() == 'exit':
        break
    query_embedding = embeddings.embed_query(query)
    result = index.query(vector=query_embedding, top_k=1, include_metadata=True)
    most_similar_chunk = result.matches[0].metadata["text"]
    print("\nAnswer:", most_similar_chunk)
